{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors in Glove 6B 100d.\n",
      "Embedding Matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phdstudents/anaconda2/lib/python2.7/site-packages/keras/layers/recurrent.py:2112: UserWarning: RNN dropout is no longer supported with the Theano backend due to technical limitations. You can either set `dropout` and `recurrent_dropout` to 0, or use the TensorFlow backend.\n",
      "  'RNN dropout is no longer supported with the Theano backend '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 100)           157500    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 100)           80400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50, 200)           240800    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50, 200)           320800    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 50, 200)           320800    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               120400    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 1,241,104\n",
      "Trainable params: 1,083,604\n",
      "Non-trainable params: 157,500\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 369 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      " - 5s - loss: 1.3162 - acc: 0.4255 - precision: nan - recall: 0.0488 - f1_score: nan - val_loss: 1.2856 - val_acc: 0.5500 - val_precision: nan - val_recall: 0.0000e+00 - val_f1_score: nan\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         4\n",
      "          1       0.00      0.00      0.00         5\n",
      "          2       0.55      1.00      0.71        11\n",
      "          3       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.30      0.55      0.39        20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phdstudents/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/phdstudents/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer lstm_6: expected ndim=3, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-626df16d98da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m    \u001b[0;31m# X_train, X_test = padded_docs[train], padded_docs[test]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;31m#Y_train, Y_test = labels[train], labels[test]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/phdstudents/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    490\u001b[0m                           output_shapes=[self.outputs[0]._keras_shape])\n\u001b[1;32m    491\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m/home/phdstudents/anaconda2/lib/python2.7/site-packages/keras/layers/recurrent.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/phdstudents/anaconda2/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/phdstudents/anaconda2/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    472\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    475\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer lstm_6: expected ndim=3, found ndim=2"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from keras.preprocessing.text import one_hot\n",
    "#from keras.preprocessing.text import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.callbacks import History\n",
    "from numpy import zeros\n",
    "from keras.layers.embeddings import Embedding\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prf1 import precision, recall, f1_score\n",
    "from keras.utils.vis_utils import plot_model\n",
    "#import pydot\n",
    "#import graphviz\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import LSTM\n",
    "import numpy\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from confusionMetrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from prf1 import precision, recall, f1_score\n",
    "from keras.utils import np_utils\n",
    "from imblearn.over_sampling import SMOTE\n",
    "#######Loading CSV file using Pandas\n",
    " \n",
    "df=pd.read_excel('Airtel_MARKED.xlsx')\n",
    "\n",
    "\n",
    "#print((df.info()))\n",
    "\n",
    "df1=df['message']\n",
    "label =df['Label']\n",
    "\n",
    "#x_val, y_val = sm.fit_sample(x_val1, y_val1)\n",
    "#print(label)\n",
    "#Y=np_utils.to_categorical(Y_old)\n",
    "#print(df1.info())\n",
    "tk=Tokenizer()\n",
    "tk.fit_on_texts(df1)\n",
    "index=tk.word_index\n",
    "#print(index)\n",
    "x = tk.texts_to_sequences(df1)\n",
    "#print (x)\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "##encoded_doc = tk.texts_to_matrix(df, mode='count')\n",
    "##print (encoded_doc)\n",
    "##max_length=15\n",
    "##padded_docs = sequence.pad_sequences(encoded_doc, maxlen=max_length, padding='pre')\n",
    "##print (padded_docs)\n",
    "vocab_size = len(index)\n",
    "\n",
    "\n",
    "#print(vocab_size)\n",
    "encoded_docs=[one_hot(d,vocab_size) for d in df1] \n",
    "#print (hello)\n",
    "##############################Padding###############\n",
    "max_length=50\n",
    "padded_docs = sequence.pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "#print padded_docs\n",
    "#####################Encoded label#######\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(label)\n",
    "encoded_label = encoder.transform(label)\n",
    "#print(encoded_label)\n",
    "labels = np_utils.to_categorical(encoded_label)\n",
    "#print(labels)\n",
    "#print('Shape of data tensor:', x_train.shape)\n",
    "#print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "\n",
    "\n",
    "#x_train1, x_val1, y_train1, y_val1 = check_X_y(padded_docs, labels, accept_sparse=\"csc\", dtype=np.float32, multi_output=True)\n",
    "#x_train1, x_val1, y_train1, y_val1=train_test_split(padded_docs,labels, test_size=0.25,random_state=42)\n",
    "#print(x_train1.shape)\n",
    "#print(x_val1.shape)\n",
    "#print(y_train1.shape)\n",
    "#print(y_val1.shape)\n",
    "#sm = SMOTE(kind='regular')\n",
    "#x_train, y_train = sm.fit_sample(x_train1,y_train1)\n",
    "#x_val,y_val=sm.fit_sample(x_val1,y_val1)\n",
    " \n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    " \n",
    "print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))\n",
    " \n",
    "# In[10]:\n",
    "embedding_matrix =zeros((vocab_size+ 1, 100))\n",
    "for word, i in tk.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector   \n",
    "print (\"Embedding Matrix\")\n",
    " \n",
    " \n",
    " \n",
    "#############embedding layers#########################\n",
    " \n",
    "history=History()\n",
    "predictions_train=[]\n",
    "predictions_test=[]\n",
    "filter_sizes = [3,4,5]\n",
    "fold_training=numpy.zeros(shape=75)\n",
    "fold_test=numpy.zeros(shape=75)\n",
    "##embedding_vecor_length =32\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size+1, 100,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length,\n",
    "                            trainable=False))\n",
    "##model.add(Embedding(vocab_size,8, input_length=embedding_vecor_length))\n",
    "kfold = KFold(n_splits=2, shuffle=True, random_state=7)\n",
    "cvscores = []\n",
    "ith=1\n",
    "for train,test in kfold.split(padded_docs,labels):\n",
    "    #print('Fold=',ith)\n",
    "    #ith=ith+1\n",
    "    #print(train,test)\n",
    "   # X_train, X_test = padded_docs[train], padded_docs[test]\n",
    "    #Y_train, Y_test = labels[train], labels[test]\n",
    "    model.add(LSTM(100,return_sequences=True))\n",
    "    model.add(LSTM(200,return_sequences=True))\n",
    "    model.add(LSTM(200,return_sequences=True))\n",
    "    model.add(LSTM(200,return_sequences=True,recurrent_dropout=0.2))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='RMSprop',\n",
    "                  metrics=['acc', precision, recall, f1_score])\n",
    "    #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', precision,recall,f1_score])\n",
    "    print(model.summary())\n",
    "    history=model.fit(padded_docs[train], labels[train],validation_data=(padded_docs[test],labels[test]),epochs=1, batch_size=20,verbose=2, callbacks=[history])\n",
    "    plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
    "    predictions = model.predict(padded_docs[test])\n",
    "    b=np.zeros_like(predictions)\n",
    "    b[np.arange(len(predictions)), predictions.argmax(1)]=1\n",
    "    print( metrics.classification_report(labels[test],b))\n",
    "\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))\n",
    "fold_training=numpy.divide(fold_training,20)\n",
    "print(fold_training)\n",
    "fold_test=numpy.divide(fold_test,20)\n",
    "print(fold_test)\n",
    "accuracy = model.evaluate(padded_docs[test],labels[test], verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "    \n",
    "    \n",
    "                               \n",
    "                               \n",
    "    \n",
    "'''history=History()\n",
    "plt.plot(history.history['precision'])\n",
    "plt.plot(history.history['val_precision'])\n",
    "plt.ylabel('precision')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'test'])\n",
    "plt.show()'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(padded_docs[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27416563, 0.19861417, 0.31205517, 0.21516505],\n",
       "       [0.27511615, 0.19561127, 0.31741408, 0.21185851],\n",
       "       [0.27222916, 0.18945298, 0.34318075, 0.19513711],\n",
       "       [0.27442098, 0.20008937, 0.3101625 , 0.21532713],\n",
       "       [0.2738794 , 0.19891837, 0.31136346, 0.21583876],\n",
       "       [0.27372432, 0.19976644, 0.3106749 , 0.21583433],\n",
       "       [0.27375105, 0.19970416, 0.3106949 , 0.21584989],\n",
       "       [0.27633438, 0.19442032, 0.32019523, 0.20905004],\n",
       "       [0.2737843 , 0.19965546, 0.31076202, 0.21579821],\n",
       "       [0.27381232, 0.19967881, 0.31073168, 0.21577719],\n",
       "       [0.27370515, 0.19982591, 0.31055412, 0.21591482],\n",
       "       [0.2736112 , 0.19979489, 0.3104157 , 0.21617827],\n",
       "       [0.273842  , 0.19937739, 0.3109517 , 0.21582891],\n",
       "       [0.27367356, 0.19977087, 0.3106066 , 0.21594895],\n",
       "       [0.27365065, 0.19961485, 0.3106595 , 0.216075  ],\n",
       "       [0.2738125 , 0.19940849, 0.31108335, 0.21569566],\n",
       "       [0.27383292, 0.19950876, 0.31093797, 0.21572036],\n",
       "       [0.27375826, 0.19969845, 0.31066176, 0.21588153],\n",
       "       [0.27369034, 0.19981521, 0.31058133, 0.21591312],\n",
       "       [0.27417296, 0.19889513, 0.31170958, 0.21522233]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
